{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a538027",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "#### From the initial analysis it was observed that meta-llama/Meta-Llama-3.1-8B-Instruct model performance was better than all the other models for both Task A and Task B with Multistage Fallacy detection. Keeping this model as fixed, we experimented with different prompts and observe their performance. For testing, we used meta-llama/Meta-Llama-3.1-8B-Instruct model, with 3-stage fallacy detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d56062",
   "metadata": {},
   "source": [
    "# Task A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85848a0",
   "metadata": {},
   "source": [
    "## Prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155254bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(text: str, stage_labels: List[str], \n",
    "                           examples: Dict[str, List[str]], \n",
    "                           stage_name: str,\n",
    "                           max_examples: int = 3) -> str:\n",
    "        \n",
    "        label_descriptions = {\n",
    "            \"Ad-hominem\": \"Attacking the person instead of the argument\",\n",
    "            \"Appeal-to-authority\": \"Using authority as evidence\",\n",
    "            \"Appeal-to-emotion\": \"Manipulating emotions instead of using logic\",\n",
    "            \"Causal-oversimplification\": \"Oversimplifying cause-effect relationships\",\n",
    "            \"Cherry-picking\": \"Selecting only favorable evidence\",\n",
    "            \"Circular-reasoning\": \"Conclusion is assumed in the premise\",\n",
    "            \"Doubt\": \"Questioning credibility without evidence\",\n",
    "            \"Evading-the-burden-of-proof\": \"Avoiding providing evidence\",\n",
    "            \"False-analogy\": \"Comparing incomparable things\",\n",
    "            \"False-dilemma\": \"Presenting only two options when more exist\",\n",
    "            \"Flag-waving\": \"Appealing to nationalism/patriotism\",\n",
    "            \"Hasty-generalization\": \"Drawing conclusions from insufficient evidence\",\n",
    "            \"Loaded-language\": \"Using emotionally charged words\",\n",
    "            \"Name-calling-or-labelling\": \"Using derogatory labels\",\n",
    "            \"Red-herring\": \"Introducing irrelevant information\",\n",
    "            \"Slippery-slope\": \"Claiming one thing will lead to extreme consequences\",\n",
    "            \"Slogan\": \"Using catchy phrases instead of arguments\",\n",
    "            \"Strawman\": \"Misrepresenting opponent's argument\",\n",
    "            \"Thought-terminating-cliches\": \"Using clichés to stop critical thinking\",\n",
    "            \"Vagueness\": \"Being intentionally unclear\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert in identifying logical fallacies and propaganda techniques in text.\n",
    "\n",
    "STAGE: {stage_name}\n",
    "\n",
    "Your task is to identify which fallacy types from the list below are present in the given text.\n",
    "ONLY consider the fallacies listed for this stage. A text can have multiple fallacies.\n",
    "\n",
    "FALLACY TYPES FOR THIS STAGE:\n",
    "\"\"\"\n",
    "        for label in stage_labels:\n",
    "            prompt += f\"- {label}: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "        \n",
    "        prompt += \"\\n\"\n",
    "        if examples:\n",
    "            prompt += \"EXAMPLES:\\n\\n\"\n",
    "            example_posts = []\n",
    "            for label in stage_labels:\n",
    "                if label in examples and examples[label]:\n",
    "                    example_posts.append((examples[label][0], label))\n",
    "            \n",
    "            import random\n",
    "            if len(example_posts) > max_examples:\n",
    "                random.seed(42)\n",
    "                example_posts = random.sample(example_posts, max_examples)\n",
    "            \n",
    "            for idx, (example_text, label) in enumerate(example_posts, 1):\n",
    "                prompt += f\"Example {idx}:\\n\"\n",
    "                prompt += f\"Text: \\\"{example_text}\\\"\\n\"\n",
    "                prompt += f\"Contains: {label}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"Now analyze this text and identify which fallacies from the {stage_name} list are present:\n",
    "\n",
    "TEXT: \"{text}\"\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. ONLY consider fallacies from the list above for this stage\n",
    "2. Identify ALL applicable fallacies (can be multiple)\n",
    "3. If no fallacies from this stage are found, return empty array\n",
    "4. Be precise - only return labels that clearly apply\n",
    "\n",
    "Output format: Return ONLY a JSON array of labels:\n",
    "[\"Label1\", \"Label2\"]\n",
    "\n",
    "If no fallacies from this stage: []\n",
    "\n",
    "Your response (JSON array only):\"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfe316",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "            prec       rec       f1\n",
    "a1          32.1      45.5     37.64\n",
    "a2          34.11     45.48    38.98\n",
    "avg         33.1      45.49    38.31\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1942ac",
   "metadata": {},
   "source": [
    "## Prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str], \n",
    "                           examples: Dict[str, List[str]], \n",
    "                           stage_name: str,\n",
    "                           max_examples: int = 3) -> str:\n",
    "        \n",
    "        label_descriptions = {\n",
    "            \"Ad-hominem\": \"Attacking the person instead of the argument\",\n",
    "            \"Appeal-to-authority\": \"Using authority as evidence\",\n",
    "            \"Appeal-to-emotion\": \"Manipulating emotions instead of using logic\",\n",
    "            \"Causal-oversimplification\": \"Oversimplifying cause-effect relationships\",\n",
    "            \"Cherry-picking\": \"Selecting only favorable evidence\",\n",
    "            \"Circular-reasoning\": \"Conclusion is assumed in the premise\",\n",
    "            \"Doubt\": \"Questioning credibility without evidence\",\n",
    "            \"Evading-the-burden-of-proof\": \"Avoiding providing evidence\",\n",
    "            \"False-analogy\": \"Comparing incomparable things\",\n",
    "            \"False-dilemma\": \"Presenting only two options when more exist\",\n",
    "            \"Flag-waving\": \"Appealing to nationalism/patriotism\",\n",
    "            \"Hasty-generalization\": \"Drawing conclusions from insufficient evidence\",\n",
    "            \"Loaded-language\": \"Using emotionally charged words\",\n",
    "            \"Name-calling-or-labelling\": \"Using derogatory labels\",\n",
    "            \"Red-herring\": \"Introducing irrelevant information\",\n",
    "            \"Slippery-slope\": \"Claiming one thing will lead to extreme consequences\",\n",
    "            \"Slogan\": \"Using catchy phrases instead of arguments\",\n",
    "            \"Strawman\": \"Misrepresenting opponent's argument\",\n",
    "            \"Thought-terminating-cliches\": \"Using clichés to stop critical thinking\",\n",
    "            \"Vagueness\": \"Being intentionally unclear\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert in identifying logical fallacies and propaganda techniques.\n",
    "\n",
    "TASK: Analyze the text below for {stage_name} fallacies. Think step-by-step.\n",
    "\n",
    "FALLACY TYPES TO CONSIDER:\n",
    "\"\"\"\n",
    "        for label in stage_labels:\n",
    "            prompt += f\"- {label}: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "\n",
    "        prompt += \"\\n\"\n",
    "\n",
    "        if examples:\n",
    "            prompt += \"EXAMPLES OF REASONING:\\n\\n\"\n",
    "            example_posts = list(examples.items())[:2]\n",
    "            for idx, (label, texts) in enumerate(example_posts, 1):\n",
    "                if texts:\n",
    "                    prompt += f\"Example {idx}:\\n\"\n",
    "                    prompt += f\"Text: \\\"{texts[0][:150]}...\\\"\\n\"\n",
    "                    prompt += f\"Reasoning: This text contains {label} because [brief explanation]\\n\"\n",
    "                    prompt += f\"Label: {label}\\n\\n\"\n",
    "\n",
    "        prompt += f\"\"\"TEXT TO ANALYZE: \"{text}\"\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Read the text carefully\n",
    "    2. For each fallacy type, ask: \"Is this fallacy clearly present?\"\n",
    "    3. Provide brief reasoning for fallacies you identify\n",
    "    4. List all applicable fallacies\n",
    "\n",
    "    FORMAT YOUR RESPONSE AS:\n",
    "    Reasoning: [Your step-by-step analysis]\n",
    "    Labels: [\"Label1\", \"Label2\"] or []\n",
    "\n",
    "    Your response:\"\"\"\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbddd1a",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "            prec       rec       f1\n",
    "a1          12.92     60.66     21.3\n",
    "a2          14.42     63.70     23.52\n",
    "avg         13.67     62.18     22.41\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c80217",
   "metadata": {},
   "source": [
    "## Prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecf8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str],\n",
    "                        examples: Dict[str, List[str]],\n",
    "                        stage_name: str,\n",
    "                        max_examples: int = 3) -> str:\n",
    "\n",
    "    label_descriptions = {\n",
    "        \"Ad-hominem\": \"Attacking the person instead of the argument\",\n",
    "        \"Appeal-to-authority\": \"Using authority as evidence\",\n",
    "        \"Appeal-to-emotion\": \"Manipulating emotions instead of using logic\",\n",
    "        \"Causal-oversimplification\": \"Oversimplifying cause-effect relationships\",\n",
    "        \"Cherry-picking\": \"Selecting only favorable evidence\",\n",
    "        \"Circular-reasoning\": \"Conclusion is assumed in the premise\",\n",
    "        \"Doubt\": \"Questioning credibility without evidence\",\n",
    "        \"Evading-the-burden-of-proof\": \"Avoiding providing evidence\",\n",
    "        \"False-analogy\": \"Comparing incomparable things\",\n",
    "        \"False-dilemma\": \"Presenting only two options when more exist\",\n",
    "        \"Flag-waving\": \"Appealing to nationalism/patriotism\",\n",
    "        \"Hasty-generalization\": \"Drawing conclusions from insufficient evidence\",\n",
    "        \"Loaded-language\": \"Using emotionally charged words\",\n",
    "        \"Name-calling-or-labelling\": \"Using derogatory labels\",\n",
    "        \"Red-herring\": \"Introducing irrelevant information\",\n",
    "        \"Slippery-slope\": \"Claiming one thing will lead to extreme consequences\",\n",
    "        \"Slogan\": \"Using catchy phrases instead of arguments\",\n",
    "        \"Strawman\": \"Misrepresenting opponent's argument\",\n",
    "        \"Thought-terminating-cliches\": \"Using clichés to stop critical thinking\",\n",
    "        \"Vagueness\": \"Being intentionally unclear\"\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"You are an expert in identifying logical fallacies and propaganda techniques.\n",
    "\n",
    "TASK: Analyze the text below for possible {stage_name} fallacies.\n",
    "\n",
    "FALLACY TYPES TO CONSIDER:\n",
    "\"\"\"\n",
    "    for label in stage_labels:\n",
    "        prompt += f\"- {label}: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "\n",
    "    prompt += \"\\n\"\n",
    "\n",
    "    if examples:\n",
    "        prompt += \"EXAMPLES (follow this exact output style):\\n\\n\"\n",
    "        count = 0\n",
    "        for label, texts in examples.items():\n",
    "            if count >= max_examples:\n",
    "                break\n",
    "            example_text = texts[0].replace(\"\\n\", \" \").strip()\n",
    "            if len(example_text) > 200:\n",
    "                example_text = example_text[:200] + \"...\"\n",
    "            prompt += f\"Example {count+1}:\\n\"\n",
    "            prompt += f'Text: \"{example_text}\"\\n'\n",
    "            prompt += f'Reasoning: This text contains {label} because [short justification].\\n'\n",
    "            prompt += f'Labels: [\"{label}\"]\\n\\n'\n",
    "            count += 1\n",
    "\n",
    "    prompt += f\"\"\"TEXT TO ANALYZE:\n",
    "\"{text}\"\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Identify only the fallacies explicitly listed above.\n",
    "2. Explain briefly why if you detect one.\n",
    "3. If none, output an empty list.\n",
    "\n",
    "FORMAT YOUR RESPONSE EXACTLY LIKE THIS:\n",
    "Reasoning: [short explanation or \"No fallacies found.\"]\n",
    "Labels: [\"Label1\", \"Label2\"]  OR  Labels: []\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9f728",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "            prec       rec       f1\n",
    "a1          24.65     42.64     31.24\n",
    "a2          28.56     46.47     35.38\n",
    "avg         26.61     44.56     33.31\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4ef08",
   "metadata": {},
   "source": [
    "## Prompt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str],\n",
    "                        examples: Dict[str, List[str]],\n",
    "                        stage_name: str,\n",
    "                        max_examples: int = 3) -> str:\n",
    "\n",
    "        label_descriptions = {\n",
    "            \"Ad-hominem\": \"Attacking the person instead of the argument\",\n",
    "            \"Appeal-to-authority\": \"Using authority as evidence\",\n",
    "            \"Appeal-to-emotion\": \"Manipulating emotions instead of using logic\",\n",
    "            \"Causal-oversimplification\": \"Oversimplifying cause-effect relationships\",\n",
    "            \"Cherry-picking\": \"Selecting only favorable evidence\",\n",
    "            \"Circular-reasoning\": \"Conclusion is assumed in the premise\",\n",
    "            \"Doubt\": \"Questioning credibility without evidence\",\n",
    "            \"Evading-the-burden-of-proof\": \"Avoiding providing evidence\",\n",
    "            \"False-analogy\": \"Comparing incomparable things\",\n",
    "            \"False-dilemma\": \"Presenting only two options when more exist\",\n",
    "            \"Flag-waving\": \"Appealing to nationalism/patriotism\",\n",
    "            \"Hasty-generalization\": \"Drawing conclusions from insufficient evidence\",\n",
    "            \"Loaded-language\": \"Using emotionally charged words\",\n",
    "            \"Name-calling-or-labelling\": \"Using derogatory labels\",\n",
    "            \"Red-herring\": \"Introducing irrelevant information\",\n",
    "            \"Slippery-slope\": \"Claiming one thing will lead to extreme consequences\",\n",
    "            \"Slogan\": \"Using catchy phrases instead of arguments\",\n",
    "            \"Strawman\": \"Misrepresenting opponent's argument\",\n",
    "            \"Thought-terminating-cliches\": \"Using clichés to stop critical thinking\",\n",
    "            \"Vagueness\": \"Being intentionally unclear\"\n",
    "        }\n",
    "\n",
    "        prompt = f\"\"\"You are a highly-attuned **Logical Fallacy and Propaganda Identifier**. Your sole function is to perform precise span-extraction for a specific set of fallacies.\n",
    "##  STAGE: {stage_name}\n",
    "\n",
    "Your task is to analyze the provided text and **extract all spans** that contain ONLY the logical fallacy types listed below for this stage.\n",
    "\n",
    "**STRICT RULE:** You **MUST IGNORE** any fallacy type not present in the list below. Focus exclusively on the defined set.\n",
    "\n",
    "### FALLACY TYPES FOR THIS STAGE:\n",
    "\"\"\"\n",
    "        for label in stage_labels:\n",
    "            prompt += f\"- **{label}**: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "\n",
    "        if examples:\n",
    "            prompt += \"EXAMPLES (follow this exact output style):\\n\\n\"\n",
    "            count = 0\n",
    "            for label, texts in examples.items():\n",
    "                if count >= max_examples:\n",
    "                    break\n",
    "                example_text = texts[0].replace(\"\\n\", \" \").strip()\n",
    "                if len(example_text) > 200:\n",
    "                    example_text = example_text[:200] + \"...\"\n",
    "                prompt += f\"Example {count+1}:\\n\"\n",
    "                prompt += f'Text: \"{example_text}\"\\n'\n",
    "                prompt += f'Reasoning: This text contains {label} because [short justification].\\n'\n",
    "                prompt += f'Labels: [\"{label}\"]\\n\\n'\n",
    "                count += 1\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "##  INPUT TEXT\n",
    "\n",
    "Analyze the following text for the **{stage_name}** fallacies:\n",
    "\n",
    "TEXT: \"{text}\"\n",
    "\n",
    "---\n",
    "\n",
    "##  CRITICAL EXTRACTION INSTRUCTIONS:\n",
    "\n",
    "1.  **SCOPE:** Identify **ALL** text spans exhibiting a fallacy from the **FALLACY TYPES FOR THIS STAGE** list.\n",
    "2.  **EXCLUSIVITY:** **DO NOT** extract or label any other fallacy type (e.g., if 'Ad-hominem' is in the text but not in the list, you must ignore it).\n",
    "3.  **PRECISION:** The extracted **\"text\"** must be the **exact, unedited substring** from the original input text.\n",
    "4.  **SPAN SIZE:** A span can be a phrase, a single sentence, or a sequence of sentences. Multiple distinct spans can share the same label.\n",
    "\n",
    "FORMAT YOUR RESPONSE EXACTLY LIKE THIS:\n",
    "Reasoning: [short explanation or \"No fallacies found.\"]\n",
    "Labels: [\"Label1\", \"Label2\"]  OR  Labels: []\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efca0de",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "            prec       rec       f1\n",
    "a1          27.93     43.99     34.17\n",
    "a2          31.36     46.47     37.45\n",
    "avg         29.65     45.23     35.81\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016afcaa",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7422501d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2ae05ea142914cc493250927d34614ba.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2ae05ea142914cc493250927d34614ba.vega-embed details,\n",
       "  #altair-viz-2ae05ea142914cc493250927d34614ba.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2ae05ea142914cc493250927d34614ba\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2ae05ea142914cc493250927d34614ba\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2ae05ea142914cc493250927d34614ba\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@6?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@6.1.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@7?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"6\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"6.1.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"7\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-3d716e7d3ff99d85760ddc98d4779951\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Prompt\", \"type\": \"nominal\"}, {\"field\": \"Metric\", \"type\": \"nominal\"}, {\"field\": \"Value\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"labelAngle\": 0}, \"field\": \"Prompt\", \"title\": \"Prompt\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Value\", \"title\": \"Score\", \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"Metrics Across Prompts\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v6.1.0.json\", \"datasets\": {\"data-3d716e7d3ff99d85760ddc98d4779951\": [{\"Prompt\": \"Prompt1\", \"Metric\": \"prec\", \"Value\": 33.1}, {\"Prompt\": \"Prompt2\", \"Metric\": \"prec\", \"Value\": 13.67}, {\"Prompt\": \"Prompt3\", \"Metric\": \"prec\", \"Value\": 26.61}, {\"Prompt\": \"Prompt4\", \"Metric\": \"prec\", \"Value\": 29.65}, {\"Prompt\": \"Prompt1\", \"Metric\": \"rec\", \"Value\": 45.49}, {\"Prompt\": \"Prompt2\", \"Metric\": \"rec\", \"Value\": 62.18}, {\"Prompt\": \"Prompt3\", \"Metric\": \"rec\", \"Value\": 44.56}, {\"Prompt\": \"Prompt4\", \"Metric\": \"rec\", \"Value\": 45.23}, {\"Prompt\": \"Prompt1\", \"Metric\": \"f1\", \"Value\": 38.31}, {\"Prompt\": \"Prompt2\", \"Metric\": \"f1\", \"Value\": 22.41}, {\"Prompt\": \"Prompt3\", \"Metric\": \"f1\", \"Value\": 33.31}, {\"Prompt\": \"Prompt4\", \"Metric\": \"f1\", \"Value\": 35.81}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "#  metrics as data\n",
    "df = pd.DataFrame({\n",
    "    \"Prompt\": [\"Prompt1\", \"Prompt2\", \"Prompt3\", \"Prompt4\"],\n",
    "    \"prec\": [33.1, 13.67, 26.61, 29.65],\n",
    "    \"rec\":  [45.49, 62.18, 44.56, 45.23],\n",
    "    \"f1\":   [38.31, 22.41, 33.31, 35.81],\n",
    "})\n",
    "\n",
    "df_melt = df.melt(id_vars=\"Prompt\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(df_melt)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"Prompt:N\", title=\"Prompt\",\n",
    "                axis=alt.Axis(labelAngle=0)),   \n",
    "        y=alt.Y(\"Value:Q\", title=\"Score\"),\n",
    "        color=\"Metric:N\",\n",
    "        tooltip=[\"Prompt\", \"Metric\", \"Value\"]\n",
    "    )\n",
    "    .properties(\n",
    "        width=500,\n",
    "        height=300,\n",
    "        title=\"Metrics Across Prompts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc4bc4",
   "metadata": {},
   "source": [
    "### Description \n",
    "\n",
    "The comparison across various prompts shows that Prompt 1 achieves the highest F1 score, \n",
    "resulting in the best overall balance between precision and recall. Eventhough, its recall is slightly\n",
    "lower than Prompt 2, Prompt 1 maintains substantially higher precision, leading to the strongest\n",
    "combined performance. \n",
    "Prompt 2 have very low precision, pulling its F1 score down despite having the highest recall. \n",
    "Prompts 3 and 4 show moderate and more stable performance but do not outperform Prompt 1. \n",
    "Overall, Prompt 1 provides the most effective and reliable results across the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c0fbd",
   "metadata": {},
   "source": [
    "# Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b828c",
   "metadata": {},
   "source": [
    "## Prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90040111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str],\n",
    "                           examples: Dict[str, List[str]],\n",
    "                           stage_name: str,\n",
    "                           max_examples_per_label: int = 3) -> str:\n",
    "        \n",
    "        label_descriptions = {\n",
    "            \"Ad-hominem\": \"Attacking the person instead of the argument\",\n",
    "            \"Appeal-to-authority\": \"Using authority as evidence\",\n",
    "            \"Appeal-to-emotion\": \"Manipulating emotions instead of using logic\",\n",
    "            \"Causal-oversimplification\": \"Oversimplifying cause-effect relationships\",\n",
    "            \"Cherry-picking\": \"Selecting only favorable evidence\",\n",
    "            \"Circular-reasoning\": \"Conclusion is assumed in the premise\",\n",
    "            \"Doubt\": \"Questioning credibility without evidence\",\n",
    "            \"Evading-the-burden-of-proof\": \"Avoiding providing evidence\",\n",
    "            \"False-analogy\": \"Comparing incomparable things\",\n",
    "            \"False-dilemma\": \"Presenting only two options when more exist\",\n",
    "            \"Flag-waving\": \"Appealing to nationalism/patriotism\",\n",
    "            \"Hasty-generalization\": \"Drawing conclusions from insufficient evidence\",\n",
    "            \"Loaded-language\": \"Using emotionally charged words\",\n",
    "            \"Name-calling-or-labelling\": \"Using derogatory labels\",\n",
    "            \"Red-herring\": \"Introducing irrelevant information\",\n",
    "            \"Slippery-slope\": \"Claiming one thing will lead to extreme consequences\",\n",
    "            \"Slogan\": \"Using catchy phrases instead of arguments\",\n",
    "            \"Strawman\": \"Misrepresenting opponent's argument\",\n",
    "            \"Thought-terminating-cliches\": \"Using clichés to stop critical thinking\",\n",
    "            \"Vagueness\": \"Being intentionally unclear\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"You are an expert in identifying logical fallacies in text.\n",
    "\n",
    "STAGE: {stage_name}\n",
    "\n",
    "Your task is to extract text spans that contain ONLY the fallacy types listed below.\n",
    "IGNORE all other fallacy types - focus only on this stage's fallacies.\n",
    "\n",
    "FALLACY TYPES FOR THIS STAGE:\n",
    "\"\"\"\n",
    "        \n",
    "        for label in stage_labels:\n",
    "            prompt += f\"- {label}: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "        \n",
    "        # Add examples\n",
    "        if examples:\n",
    "            prompt += \"\\nEXAMPLES:\\n\"\n",
    "            for label in stage_labels:\n",
    "                if label in examples and examples[label]:\n",
    "                    prompt += f\"\\n{label}:\\n\"\n",
    "                    for ex in examples[label][:max_examples_per_label]:\n",
    "                        prompt += f'  - \"{ex}\"\\n'\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "Now extract spans containing ONLY the {stage_name} fallacies from this text:\n",
    "\n",
    "TEXT: \"{text}\"\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Extract ALL spans containing fallacies from the list above\n",
    "2. IGNORE fallacies not in this stage's list\n",
    "3. A span can be a phrase, sentence, or multiple sentences\n",
    "4. Multiple spans can have the same label\n",
    "5. Return exact text from the input\n",
    "\n",
    "Output format (JSON only):\n",
    "{{\"spans\": [{{\"text\": \"exact text span\", \"label\": \"Label-name\"}}]}}\n",
    "\n",
    "If no fallacies from this stage: {{\"spans\": []}}\n",
    "\n",
    "Your response (JSON only):\"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343252d0",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "[strict evaluation]\n",
    "        prec    rec     f1\n",
    "a1      20.03   10.7    13.95\n",
    "a2      20.17   10.18   13.53\n",
    "avg     20.1    10.44   13.74\n",
    "\n",
    "[soft evaluation]\n",
    "        prec    rec     f1\n",
    "a1      27.19   14.73   19.11\n",
    "a2      27.37   14.35   18.83\n",
    "avg     27.28   14.54   18.97\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45fcd67",
   "metadata": {},
   "source": [
    "## Prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str],\n",
    "                        examples: Dict[str, List[str]],\n",
    "                        stage_name: str,\n",
    "                        max_examples_per_label: int = 3) -> str:\n",
    "\n",
    "    label_descriptions = {\n",
    "        \"Ad-hominem\": \"Attacking the speaker instead of the argument\",\n",
    "        \"Appeal-to-authority\": \"Claiming something is true just because an authority says so\",\n",
    "        \"Appeal-to-emotion\": \"Manipulating feelings instead of providing logic\",\n",
    "        \"Causal-oversimplification\": \"Reducing complex cause-effect relationships to a simple claim\",\n",
    "        \"Cherry-picking\": \"Mentioning only evidence that supports a claim\",\n",
    "        \"Circular-reasoning\": \"The conclusion is also used as a premise\",\n",
    "        \"Doubt\": \"Casting suspicion without offering evidence\",\n",
    "        \"Evading-the-burden-of-proof\": \"Making claims without backing them up\",\n",
    "        \"False-analogy\": \"Comparing two unrelated things as if they are similar\",\n",
    "        \"False-dilemma\": \"Claiming only two options exist when more do\",\n",
    "        \"Flag-waving\": \"Appealing to patriotism or national identity\",\n",
    "        \"Hasty-generalization\": \"Concluding based on very limited data\",\n",
    "        \"Loaded-language\": \"Using emotionally charged language\",\n",
    "        \"Name-calling-or-labelling\": \"Using insult or label instead of argument\",\n",
    "        \"Red-herring\": \"Bringing up irrelevant information to distract\",\n",
    "        \"Slippery-slope\": \"Claiming one event will definitely lead to a dramatic outcome\",\n",
    "        \"Slogan\": \"Repeating a catchy phrase instead of reasoning\",\n",
    "        \"Strawman\": \"Distorting someone’s argument to attack it more easily\",\n",
    "        \"Thought-terminating-cliches\": \"Phrases that shut down critical thinking\",\n",
    "        \"Vagueness\": \"Statements that are unclear and undefined\"\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"You are a fallacy detection expert.\n",
    "\n",
    "STAGE: {stage_name}\n",
    "\n",
    "GOAL:\n",
    "Extract ONLY spans of text that contain one or more of the fallacy types listed below.\n",
    "Do NOT mark anything else as a fallacy.\n",
    "\n",
    "FALLACY TYPES IN THIS STAGE:\n",
    "\"\"\"\n",
    "    for label in stage_labels:\n",
    "        prompt += f\"- {label}: {label_descriptions.get(label, 'Fallacy type')}\\n\"\n",
    "\n",
    "    # Few-shot examples section\n",
    "    if examples:\n",
    "        prompt += \"\\nREFERENCE EXAMPLES (Learn the style):\\n\"\n",
    "        for label in stage_labels:\n",
    "            if label in examples and examples[label]:\n",
    "                prompt += f\"\\n{label} examples:\\n\"\n",
    "                for ex in examples[label][:max_examples_per_label]:\n",
    "                    clean = ex.replace(\"\\n\", \" \").strip()\n",
    "                    prompt += f' • \"{clean}\"\\n'\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "TEXT TO ANALYZE:\n",
    "\"{text}\"\n",
    "\n",
    "IMPORTANT RULES:\n",
    "Extract spans ONLY if they fit a fallacy in this STAGE\n",
    "Spans must be copied EXACTLY as written in the text\n",
    "A text may contain multiple fallacies or none\n",
    "Include a separate entry for each labeled span\n",
    "Be precise — avoid over-long spans\n",
    "\n",
    "EXPECTED OUTPUT FORMAT — JSON ONLY:\n",
    "{{\n",
    "  \"spans\": [\n",
    "    {{\n",
    "      \"text\": \"exact span copied from text\",\n",
    "      \"label\": \"Label-name\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no valid fallacies are found:\n",
    "{{\"spans\": []}}\n",
    "\n",
    "DO NOT OUTPUT ANYTHING OTHER THAN THE JSON.\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e66cbe",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "[strict evaluation]\n",
    "        prec    rec     f1\n",
    "a1      14.39   20.11   16.78\n",
    "a2      13.45   19.83   16.02\n",
    "avg     13.92   19.97   16.4\n",
    "\n",
    "[soft evaluation]\n",
    "        prec     rec     f1\n",
    "a1      17.31   25.56   20.64\n",
    "a2      16.82   26.5    20.58\n",
    "avg     17.06   26.03   20.61\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd6628",
   "metadata": {},
   "source": [
    "## Prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed469bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage_prompt(self, text: str, stage_labels: List[str],\n",
    "                               examples: Dict[str, List[str]],\n",
    "                               stage_name: str,\n",
    "                               max_examples_per_label: int = 3) -> str:\n",
    "\n",
    "        label_descriptions = {\n",
    "            \"Ad-hominem\": \"Attacking the speaker instead of the argument\",\n",
    "            \"Appeal-to-authority\": \"Claiming something is true just because an authority says so\",\n",
    "            \"Appeal-to-emotion\": \"Manipulating feelings instead of providing logic\",\n",
    "            \"Causal-oversimplification\": \"Reducing complex cause-effect relationships to a simple claim\",\n",
    "            \"Cherry-picking\": \"Mentioning only evidence that supports a claim\",\n",
    "            \"Circular-reasoning\": \"The conclusion is also used as a premise\",\n",
    "            \"Doubt\": \"Casting suspicion without offering evidence\",\n",
    "            \"Evading-the-burden-of-proof\": \"Making claims without backing them up\",\n",
    "            \"False-analogy\": \"Comparing two unrelated things as if they are similar\",\n",
    "            \"False-dilemma\": \"Claiming only two options exist when more do\",\n",
    "            \"Flag-waving\": \"Appealing to patriotism or national identity\",\n",
    "            \"Hasty-generalization\": \"Concluding based on very limited data\",\n",
    "            \"Loaded-language\": \"Using emotionally charged language\",\n",
    "            \"Name-calling-or-labelling\": \"Using insult or label instead of argument\",\n",
    "            \"Red-herring\": \"Bringing up irrelevant information to distract\",\n",
    "            \"Slippery-slope\": \"Claiming one event will definitely lead to a dramatic outcome\",\n",
    "            \"Slogan\": \"Repeating a catchy phrase instead of reasoning\",\n",
    "            \"Strawman\": \"Distorting someone’s argument to attack it more easily\",\n",
    "            \"Thought-terminating-cliches\": \"Phrases that shut down critical thinking\",\n",
    "            \"Vagueness\": \"Statements that are unclear and undefined\"\n",
    "\n",
    "        }\n",
    "\n",
    "        prompt = f\"\"\"You are an expert in identifying logical fallacies in text.\n",
    "\n",
    "    STAGE: {stage_name}\n",
    "\n",
    "    Target Fallacies:\n",
    "    \"\"\"\n",
    "        for label in stage_labels:\n",
    "            prompt += f\"- {label}: {label_descriptions.get(label, 'Propaganda technique')}\\n\"\n",
    "\n",
    "        if examples:\n",
    "            prompt += \"\\nExamples:\\n\"\n",
    "            for label in stage_labels:\n",
    "                if label in examples and examples[label]:\n",
    "                    prompt += f\"\\n{label}:\\n\"\n",
    "                    for ex in examples[label][:max_examples_per_label]:\n",
    "                        prompt += f'  - \"{ex}\"\\n'\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "    TEXT TO ANALYZE:\n",
    "    \"{text}\"\n",
    "\n",
    "    ANALYSIS PROCESS:\n",
    "    1. First, read the entire text carefully\n",
    "    2. For each sentence/phrase, ask: \"Does this match any of the {len(stage_labels)} target fallacies?\"\n",
    "    3. If yes, identify which specific fallacy and extract the exact span\n",
    "    4. Ignore any fallacies NOT in the target list above\n",
    "    5. Compile all matches\n",
    "\n",
    "    Think through your analysis step by step, then provide the final JSON output:\n",
    "\n",
    "    {{\"reasoning\": \"brief explanation of what you found\", \"spans\": [{{\"text\": \"exact span\", \"label\": \"Label-name\"}}]}}\n",
    "\n",
    "    Your response:\"\"\"\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcc62c",
   "metadata": {},
   "source": [
    "## Result\n",
    "<pre>\n",
    "[strict evaluation]\n",
    "        prec     rec     f1\n",
    "a1      17.5    19.84   18.6\n",
    "a2      17.19   19.88   18.44\n",
    "avg     17.34   19.86   18.52\n",
    "\n",
    "[soft evaluation]\n",
    "        prec     rec     f1\n",
    "a1      21.85   26.13   23.8\n",
    "a2      21.79   26.79   24.03\n",
    "avg     21.82   26.46   23.92\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169b4b4",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e321c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ebc9aea3f18e4b33ad52715cd828204c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ebc9aea3f18e4b33ad52715cd828204c.vega-embed details,\n",
       "  #altair-viz-ebc9aea3f18e4b33ad52715cd828204c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ebc9aea3f18e4b33ad52715cd828204c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ebc9aea3f18e4b33ad52715cd828204c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ebc9aea3f18e4b33ad52715cd828204c\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@6?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@6.1.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@7?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"6\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"6.1.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"7\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-e6e8b07d87ad9ea2e88f5be27140a64e\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"scale\": {\"range\": [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]}, \"title\": \"Metric\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Prompt\", \"type\": \"nominal\"}, {\"field\": \"Metric\", \"type\": \"nominal\"}, {\"field\": \"Value\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"labelAngle\": 0}, \"field\": \"Prompt\", \"title\": \"Prompt\", \"type\": \"nominal\"}, \"y\": {\"field\": \"Value\", \"title\": \"Score\", \"type\": \"quantitative\"}}, \"height\": 300, \"title\": \"Metrics Across Prompts\", \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v6.1.0.json\", \"datasets\": {\"data-e6e8b07d87ad9ea2e88f5be27140a64e\": [{\"Prompt\": \"Prompt1\", \"Metric\": \"prec\", \"Value\": 27.28}, {\"Prompt\": \"Prompt2\", \"Metric\": \"prec\", \"Value\": 17.06}, {\"Prompt\": \"Prompt3\", \"Metric\": \"prec\", \"Value\": 21.82}, {\"Prompt\": \"Prompt1\", \"Metric\": \"rec\", \"Value\": 14.54}, {\"Prompt\": \"Prompt2\", \"Metric\": \"rec\", \"Value\": 26.03}, {\"Prompt\": \"Prompt3\", \"Metric\": \"rec\", \"Value\": 26.46}, {\"Prompt\": \"Prompt1\", \"Metric\": \"f1\", \"Value\": 18.97}, {\"Prompt\": \"Prompt2\", \"Metric\": \"f1\", \"Value\": 20.61}, {\"Prompt\": \"Prompt3\", \"Metric\": \"f1\", \"Value\": 23.92}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "#  metrics as data considering soft metrics\n",
    "df = pd.DataFrame({\n",
    "    \"Prompt\": [\"Prompt1\", \"Prompt2\", \"Prompt3\"],\n",
    "    \"prec\": [27.28, 17.06, 21.82],\n",
    "    \"rec\":  [14.54, 26.03, 26.46],\n",
    "    \"f1\":   [18.97, 20.61, 23.92],\n",
    "})\n",
    "\n",
    "df_melt = df.melt(id_vars=\"Prompt\", var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(df_melt)\n",
    "    .mark_line(point=True)\n",
    "    .encode(\n",
    "        x=alt.X(\"Prompt:N\", title=\"Prompt\",\n",
    "                axis=alt.Axis(labelAngle=0)),\n",
    "        y=alt.Y(\"Value:Q\", title=\"Score\"),\n",
    "        color=alt.Color(\n",
    "            \"Metric:N\",\n",
    "            scale=alt.Scale(\n",
    "                range=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  \n",
    "            ),\n",
    "            title=\"Metric\"\n",
    "        ),\n",
    "        tooltip=[\"Prompt\", \"Metric\", \"Value\"]\n",
    "    )\n",
    "    .properties(\n",
    "        width=500,\n",
    "        height=300,\n",
    "        title=\"Metrics Across Prompts\"\n",
    "    )\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4d878",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The comparison across the three prompts for Task B highlights distinct trade-offs among precision, recall, and F1 performance. Prompt 3 shows the strongest overall balance, achieving the highest F1 score alongside the highest recall, indicating consistently effective detection of fallacies across various labels. Prompt 1, shows solid precision and a competitive F1 score, but lags behind in recall, suggesting that it detects fewer relevant fallacies. Prompt 2 exhibits mixed performance: its recall rises sharply compared to Prompt 1, but this gain is offset by a noticeable drop in precision, which keeps its F1 score lower than those of Prompts 1 and 3.\n",
    "\n",
    "Overall, Prompt 3 demonstrated the most reliable and well-rounded performance across the evaluated metrics, with Prompt 1 following closely and Prompt 2 showing more variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19d5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fadeit",
   "language": "python",
   "name": "fadeit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
